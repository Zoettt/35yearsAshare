#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´æ•°æ®ä¸‹è½½è„šæœ¬ - ä»1990å¹´å¼€å§‹
é›†æˆç½‘ç»œä¿®å¤ï¼Œä¸‹è½½å…¨éƒ¨è‚¡ç¥¨å’ŒæŒ‡æ•°æ•°æ®
"""

import os
import sys
import time
from datetime import datetime

def apply_network_fix():
    """åº”ç”¨ç½‘ç»œä¿®å¤"""
    print("ğŸ”§ åº”ç”¨ç½‘ç»œä¿®å¤...")
    
    # æ¸…é™¤ä»£ç†è®¾ç½®
    proxy_vars = ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy', 'ALL_PROXY', 'all_proxy']
    for var in proxy_vars:
        if var in os.environ:
            del os.environ[var]
    
    os.environ['HTTP_PROXY'] = ''
    os.environ['HTTPS_PROXY'] = ''
    os.environ['NO_PROXY'] = '*'
    
    # ä¿®è¡¥requests
    import requests
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    original_request = requests.request
    def patched_request(*args, **kwargs):
        kwargs['proxies'] = {}
        kwargs['verify'] = False
        kwargs['timeout'] = kwargs.get('timeout', 30)
        return original_request(*args, **kwargs)
    
    requests.request = patched_request
    requests.get = lambda *args, **kwargs: patched_request('GET', *args, **kwargs)
    
    print("âœ… ç½‘ç»œä¿®å¤å·²åº”ç”¨")

def clear_kline_data():
    """æ¸…ç†Kçº¿æ•°æ®"""
    print("\nğŸ—‘ï¸ æ¸…ç†Kçº¿æ•°æ®...")
    
    # æ¸…ç†Kçº¿æ•°æ®
    kline_db = "output/kline_data/a_share_klines.db"
    kline_progress = "output/kline_data/download_progress.json"
    
    if os.path.exists(kline_db):
        os.remove(kline_db)
        print("  âœ… å·²åˆ é™¤Kçº¿æ•°æ®åº“")
    
    if os.path.exists(kline_progress):
        # å¤‡ä»½è¿›åº¦æ–‡ä»¶
        backup_name = f"output/kline_data/download_progress_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.rename(kline_progress, backup_name)
        print(f"  âœ… å·²å¤‡ä»½Kçº¿è¿›åº¦æ–‡ä»¶åˆ°: {backup_name}")
    
    print("âœ… Kçº¿æ•°æ®æ¸…ç†å®Œæˆ")

def download_index_data():
    """ä¸‹è½½æŒ‡æ•°æ•°æ®"""
    print("\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šä¸‹è½½æŒ‡æ•°æ•°æ®...")
    print("=" * 50)
    
    from major_index_fetcher import MajorIndexFetcher
    
    # åˆ›å»ºæŒ‡æ•°è·å–å™¨
    fetcher = MajorIndexFetcher(start_date="1990-01-01")
    
    try:
        # å¼€å§‹ä¸‹è½½æ‰€æœ‰æŒ‡æ•°
        fetcher.download_all_indices()
        print("âœ… æŒ‡æ•°æ•°æ®ä¸‹è½½å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ æŒ‡æ•°æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
        return False

def download_kline_data():
    """ä¸‹è½½Kçº¿æ•°æ®"""
    print("\nğŸ“ˆ ç¬¬äºŒæ­¥ï¼šä¸‹è½½Kçº¿æ•°æ®...")
    print("=" * 50)
    
    # é‡æ–°åº”ç”¨ç½‘ç»œä¿®å¤ï¼ˆç¡®ä¿åœ¨å¯¼å…¥æ¨¡å—å‰ç”Ÿæ•ˆï¼‰
    apply_network_fix()
    
    from a_share_kline_fetcher import AShareKlineFetcher
    
    # åˆ›å»ºKçº¿è·å–å™¨
    fetcher = AShareKlineFetcher(start_date="1990-01-01")
    
    print("âš™ï¸ ä¸‹è½½é…ç½®:")
    print("  - èµ·å§‹æ—¥æœŸ: 1990-01-01")
    print("  - å¹¶å‘çº¿ç¨‹: 3")
    print("  - æ‰¹æ¬¡å¤§å°: 50")
    
    try:
        # æµ‹è¯•ç½‘ç»œè¿æ¥
        print("\nğŸ§ª æµ‹è¯•Kçº¿æ•°æ®è·å–...")
        test_df = fetcher.get_kline_data_akshare('000001', '1990-01-01')
        if test_df is not None and not test_df.empty:
            print(f"âœ… ç½‘ç»œæµ‹è¯•æˆåŠŸï¼Œè·å–åˆ°æµ‹è¯•æ•°æ® {len(test_df)} æ¡")
        else:
            print("âŒ ç½‘ç»œæµ‹è¯•å¤±è´¥ï¼Œæ— æ³•è·å–æ•°æ®")
            return False
        
        # å¼€å§‹ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨
        fetcher.download_all_stocks(max_workers=3, batch_size=50)
        print("âœ… Kçº¿æ•°æ®ä¸‹è½½å®Œæˆ")
        return True
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        print("ğŸ’¾ è¿›åº¦å·²è‡ªåŠ¨ä¿å­˜ï¼Œå¯ç¨åç»§ç»­")
        return False
    except Exception as e:
        print(f"âŒ Kçº¿æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
        return False

def verify_data():
    """éªŒè¯ä¸‹è½½çš„æ•°æ®"""
    print("\nğŸ” éªŒè¯ä¸‹è½½çš„æ•°æ®...")
    
    import sqlite3
    import pandas as pd
    
    # éªŒè¯Kçº¿æ•°æ®
    try:
        conn = sqlite3.connect("output/kline_data/a_share_klines.db")
        total = pd.read_sql_query('SELECT COUNT(*) as count FROM kline_data', conn)
        date_range = pd.read_sql_query('SELECT MIN(date) as min_date, MAX(date) as max_date FROM kline_data', conn)
        stocks = pd.read_sql_query('SELECT COUNT(DISTINCT symbol) as count FROM kline_data', conn)
        pre_2015 = pd.read_sql_query('SELECT COUNT(*) as count FROM kline_data WHERE date < "2015-01-01"', conn)
        conn.close()
        
        print(f"\nğŸ“ˆ Kçº¿æ•°æ®:")
        print(f"  æ€»æ•°æ®: {total.iloc[0]['count']:,} æ¡")
        print(f"  è‚¡ç¥¨æ•°: {stocks.iloc[0]['count']} åª")
        print(f"  æ—¥æœŸèŒƒå›´: {date_range.iloc[0]['min_date']} åˆ° {date_range.iloc[0]['max_date']}")
        print(f"  2015å¹´å‰: {pre_2015.iloc[0]['count']:,} æ¡")
        
    except Exception as e:
        print(f"âŒ Kçº¿æ•°æ®éªŒè¯å¤±è´¥: {e}")
    
    # éªŒè¯æŒ‡æ•°æ•°æ®
    try:
        conn = sqlite3.connect("output/index_data/major_indices.db")
        total = pd.read_sql_query('SELECT COUNT(*) as count FROM index_data', conn)
        date_range = pd.read_sql_query('SELECT MIN(date) as min_date, MAX(date) as max_date FROM index_data', conn)
        indices = pd.read_sql_query('SELECT COUNT(DISTINCT index_name) as count FROM index_data', conn)
        pre_2015 = pd.read_sql_query('SELECT COUNT(*) as count FROM index_data WHERE date < "2015-01-01"', conn)
        conn.close()
        
        print(f"\nğŸ“Š æŒ‡æ•°æ•°æ®:")
        print(f"  æ€»æ•°æ®: {total.iloc[0]['count']:,} æ¡")
        print(f"  æŒ‡æ•°æ•°: {indices.iloc[0]['count']} ä¸ª")
        print(f"  æ—¥æœŸèŒƒå›´: {date_range.iloc[0]['min_date']} åˆ° {date_range.iloc[0]['max_date']}")
        print(f"  2015å¹´å‰: {pre_2015.iloc[0]['count']:,} æ¡")
        
    except Exception as e:
        print(f"âŒ æŒ‡æ•°æ•°æ®éªŒè¯å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ Kçº¿æ•°æ®ä¸‹è½½è„šæœ¬ - ä»1990å¹´å¼€å§‹")
    print("ğŸ“ˆ ä¸‹è½½å…¨éƒ¨Aè‚¡è‚¡ç¥¨å†å²æ•°æ®")
    print("=" * 60)
    
    print("\nâš ï¸ é‡è¦æé†’:")
    print("- ä¸‹è½½å…¨éƒ¨è‚¡ç¥¨æ•°æ®éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆæ•°å°æ—¶ï¼‰")
    print("- å»ºè®®åœ¨ç½‘ç»œç¨³å®šä¸”æ—¶é—´å……è£•æ—¶è¿è¡Œ")
    print("- æ”¯æŒä¸­æ–­åæ–­ç‚¹ç»­ä¼ ")
    print("- ä¼šå…ˆæ¸…ç†ç°æœ‰Kçº¿æ•°æ®ï¼Œé‡æ–°ä¸‹è½½å®Œæ•´å†å²æ•°æ®")
    print("- æŒ‡æ•°æ•°æ®å·²å­˜åœ¨ï¼Œå°†è·³è¿‡æŒ‡æ•°æ•°æ®ä¸‹è½½")
    
    confirm = input("\nâœ… ç¡®è®¤å¼€å§‹Kçº¿æ•°æ®ä¸‹è½½ï¼Ÿ[y/n]: ").lower().strip()
    if confirm != 'y':
        print("å–æ¶ˆä¸‹è½½")
        return
    
    start_time = datetime.now()
    
    # åº”ç”¨ç½‘ç»œä¿®å¤
    apply_network_fix()
    
    # æ¸…ç†Kçº¿æ•°æ®
    clear_kline_data()
    
    # è·³è¿‡æŒ‡æ•°æ•°æ®ä¸‹è½½ï¼ˆå·²å­˜åœ¨ï¼‰
    print("\nğŸ“Š æŒ‡æ•°æ•°æ®: âœ… å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
    
    # ä¸‹è½½Kçº¿æ•°æ®
    kline_success = download_kline_data()
    
    # éªŒè¯æ•°æ®
    verify_data()
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ ä¸‹è½½å®Œæˆæ€»ç»“:")
    print(f"  æŒ‡æ•°æ•°æ®: âœ… å·²å­˜åœ¨")
    print(f"  Kçº¿æ•°æ®: {'âœ… æˆåŠŸ' if kline_success else 'âŒ å¤±è´¥'}")
    print(f"  æ€»è€—æ—¶: {duration}")
    
    if kline_success:
        print("\nğŸ‰ Kçº¿æ•°æ®ä¸‹è½½æˆåŠŸï¼")
        print("\nğŸ“ æ•°æ®ä¿å­˜ä½ç½®:")
        print("  - Kçº¿æ•°æ®: output/kline_data/a_share_klines.db")
        print("  - æŒ‡æ•°æ•°æ®: output/index_data/major_indices.db")
        print("\nğŸŒ å¯åŠ¨Webåº”ç”¨æŸ¥çœ‹æ•°æ®: ./start_web_app.sh")
    else:
        print("\nâš ï¸ Kçº¿æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")
    
    print("=" * 60)

if __name__ == "__main__":
    main()
